{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 00:04:39.325322: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-16 00:04:39.376491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 00:04:40.365886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = load_img(os.path.join(folder, filename), color_mode='grayscale', target_size=(155, 220))\n",
    "        if img is not None:\n",
    "            images.append(img_to_array(img))\n",
    "    return np.array(images)\n",
    "\n",
    "def load_dataset(main_directory):\n",
    "    forge_images = []\n",
    "    real_images = []\n",
    "    for i in range(1, 5):\n",
    "        forge_folder = os.path.join(main_directory, f'dataset{i}', 'forge')\n",
    "        real_folder = os.path.join(main_directory, f'dataset{i}', 'real')\n",
    "        forge_images.extend(load_images_from_folder(forge_folder))\n",
    "        real_images.extend(load_images_from_folder(real_folder))\n",
    "    forge_images = np.array(forge_images)\n",
    "    real_images = np.array(real_images)\n",
    "    labels = np.array([0]*len(forge_images) + [1]*len(real_images))\n",
    "    images = np.concatenate((forge_images, real_images), axis=0)\n",
    "    return images, labels\n",
    "\n",
    "dataset_dir = \"Dataset\"\n",
    "images, labels = load_dataset(dataset_dir)\n",
    "images = images / 255.0  # Normalize images\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 00:05:41.490940: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-16 00:05:41.577411: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "def build_signet_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional Layers\n",
    "    x = Conv2D(96, (11, 11), strides=(4, 4), activation='relu')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(256, (5, 5), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(384, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(384, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "input_shape = (155, 220, 1)\n",
    "sig_model = build_signet_model(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 463ms/step - accuracy: 0.5422 - loss: 0.7261 - val_accuracy: 0.4896 - val_loss: 0.6931\n",
      "Epoch 2/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 458ms/step - accuracy: 0.5078 - loss: 0.6935 - val_accuracy: 0.4965 - val_loss: 0.6931\n",
      "Epoch 3/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 459ms/step - accuracy: 0.5020 - loss: 0.6933 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 4/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 461ms/step - accuracy: 0.4972 - loss: 0.6932 - val_accuracy: 0.5104 - val_loss: 0.6930\n",
      "Epoch 5/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 466ms/step - accuracy: 0.5016 - loss: 0.6947 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 6/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 462ms/step - accuracy: 0.5037 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 7/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 458ms/step - accuracy: 0.4719 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 8/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 456ms/step - accuracy: 0.5009 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 466ms/step - accuracy: 0.5045 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 10/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 464ms/step - accuracy: 0.5195 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fab74ca69d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def siamese_model(base_model, input_shape):\n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "\n",
    "    processed_a = base_model(input_a)\n",
    "    processed_b = base_model(input_b)\n",
    "\n",
    "    L1_layer = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([processed_a, processed_b])\n",
    "\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "    model = Model(inputs=[input_a, input_b], outputs=prediction)\n",
    "    return model\n",
    "\n",
    "siamese_net = siamese_model(sig_model, input_shape)\n",
    "\n",
    "siamese_net.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Generating pairs\n",
    "def make_pairs(images, labels):\n",
    "    pair_images = []\n",
    "    pair_labels = []\n",
    "    num_classes = len(np.unique(labels))\n",
    "    idx = [np.where(labels == i)[0] for i in range(num_classes)]\n",
    "    for idxA in range(len(images)):\n",
    "        current_image = images[idxA]\n",
    "        label = labels[idxA]\n",
    "        idxB = np.random.choice(idx[label])\n",
    "        pos_image = images[idxB]\n",
    "        \n",
    "        pair_images.append([current_image, pos_image])\n",
    "        pair_labels.append([1])\n",
    "        \n",
    "        neg_label = np.random.choice(list(set(range(num_classes)) - set([label])))\n",
    "        idxB = np.random.choice(idx[neg_label])\n",
    "        neg_image = images[idxB]\n",
    "\n",
    "        pair_images.append([current_image, neg_image])\n",
    "        pair_labels.append([0])\n",
    "    \n",
    "    return np.array(pair_images), np.array(pair_labels)\n",
    "\n",
    "pairs_train, labels_train = make_pairs(X_train, y_train)\n",
    "pairs_test, labels_test = make_pairs(X_test, y_test)\n",
    "\n",
    "siamese_net.fit([pairs_train[:, 0], pairs_train[:, 1]], labels_train, validation_data=([pairs_test[:, 0], pairs_test[:, 1]], labels_test), batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "siamese_net.save('signet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=(['<KerasTensor shape=(None, 128), dtype=float32, sparse=False, name=keras_tensor_173>', '<KerasTensor shape=(None, 128), dtype=float32, sparse=False, name=keras_tensor_175>'],)\n  • kwargs={'mask': ['None', 'None']}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     img \u001b[38;5;241m=\u001b[39m img \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msignet_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_signature\u001b[39m(img1_path, img2_path):\n\u001b[1;32m     11\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m preprocess_image(img1_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/saving/saving_api.py:183\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    177\u001b[0m         filepath,\n\u001b[1;32m    178\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    180\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/legacy/saving/legacy_h5_format.py:133\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    130\u001b[0m model_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(model_config)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m saving_options\u001b[38;5;241m.\u001b[39mkeras_option_scope(use_legacy_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 133\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     load_weights_from_hdf5_group(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], model)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/legacy/saving/saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[1;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/legacy/saving/serialization.py:495\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    490\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(\n\u001b[1;32m    491\u001b[0m     cls_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    492\u001b[0m )\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_objects\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m arg_spec\u001b[38;5;241m.\u001b[39margs:\n\u001b[0;32m--> 495\u001b[0m     deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobject_registration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGLOBAL_CUSTOM_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m object_registration\u001b[38;5;241m.\u001b[39mCustomObjectScope(custom_objects):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/models/model.py:517\u001b[0m, in \u001b[0;36mModel.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/models/functional.py:536\u001b[0m, in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    534\u001b[0m node_data \u001b[38;5;241m=\u001b[39m node_data_list[node_index]\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     \u001b[43mprocess_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# If the node does not have all inbound layers\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# available, stop processing and continue later\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/models/functional.py:483\u001b[0m, in \u001b[0;36mfunctional_from_config.<locals>.process_node\u001b[0;34m(layer, node_data)\u001b[0m\n\u001b[1;32m    480\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m deserialize_node(node_data, created_layers)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Call layer on its inputs, thus creating the node\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# and building the layer if needed.\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/layers/core/lambda_layer.py:95\u001b[0m, in \u001b[0;36mLambda.compute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mshape, output_spec)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     96\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe could not automatically infer the shape of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe Lambda\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms output. Please specify the `output_shape` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument for this Lambda layer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape(input_shape)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=(['<KerasTensor shape=(None, 128), dtype=float32, sparse=False, name=keras_tensor_173>', '<KerasTensor shape=(None, 128), dtype=float32, sparse=False, name=keras_tensor_175>'],)\n  • kwargs={'mask': ['None', 'None']}"
     ]
    }
   ],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, color_mode='grayscale', target_size=(155, 220,1))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "model = load_model('signet_model.h5')\n",
    "\n",
    "def evaluate_signature(img1_path, img2_path):\n",
    "    img1 = preprocess_image(img1_path)\n",
    "    img2 = preprocess_image(img2_path)\n",
    "    prediction = model.predict([img1, img2])\n",
    "    return prediction[0][0] * 100\n",
    "\n",
    "fake_img_path = 'signature1.jpeg'\n",
    "real_img_path = 'signature2.png'\n",
    "genuineness = evaluate_signature(fake_img_path, real_img_path)\n",
    "print(f\"Genuineness: {genuineness:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 455ms/step - accuracy: 0.4624 - loss: 0.7055 - val_accuracy: 0.5000 - val_loss: 0.6933\n",
      "Epoch 2/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 451ms/step - accuracy: 0.4821 - loss: 0.6935 - val_accuracy: 0.4826 - val_loss: 0.6932\n",
      "Epoch 3/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 449ms/step - accuracy: 0.4995 - loss: 0.6933 - val_accuracy: 0.5104 - val_loss: 0.6931\n",
      "Epoch 4/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 447ms/step - accuracy: 0.5180 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 5/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 446ms/step - accuracy: 0.5055 - loss: 0.6932 - val_accuracy: 0.5104 - val_loss: 0.6932\n",
      "Epoch 6/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 445ms/step - accuracy: 0.5022 - loss: 0.6937 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 7/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 446ms/step - accuracy: 0.4915 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 8/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 443ms/step - accuracy: 0.5052 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 445ms/step - accuracy: 0.5004 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 10/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 445ms/step - accuracy: 0.4985 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'signature2.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 124\u001b[0m\n\u001b[1;32m    122\u001b[0m fake_img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignature1.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    123\u001b[0m real_img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignature2.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 124\u001b[0m genuineness \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_img_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_img_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenuineness: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenuineness\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 117\u001b[0m, in \u001b[0;36mevaluate_signature\u001b[0;34m(img1_path, img2_path)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_signature\u001b[39m(img1_path, img2_path):\n\u001b[1;32m    116\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m preprocess_image(img1_path)\n\u001b[0;32m--> 117\u001b[0m     img2 \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg2_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([img1, img2])\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[0;32mIn[7], line 106\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(image_path):\n\u001b[0;32m--> 106\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrayscale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m155\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m220\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     img \u001b[38;5;241m=\u001b[39m img_to_array(img)\n\u001b[1;32m    108\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/image_utils.py:235\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[1;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    236\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'signature2.png'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to load images from a folder\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = load_img(os.path.join(folder, filename), color_mode='grayscale', target_size=(155, 220))\n",
    "        if img is not None:\n",
    "            images.append(img_to_array(img))\n",
    "    return np.array(images)\n",
    "\n",
    "# Function to load the dataset\n",
    "def load_dataset(main_directory):\n",
    "    forge_images = []\n",
    "    real_images = []\n",
    "    for i in range(1, 5):\n",
    "        forge_folder = os.path.join(main_directory, f'dataset{i}', 'forge')\n",
    "        real_folder = os.path.join(main_directory, f'dataset{i}', 'real')\n",
    "        forge_images.extend(load_images_from_folder(forge_folder))\n",
    "        real_images.extend(load_images_from_folder(real_folder))\n",
    "    forge_images = np.array(forge_images)\n",
    "    real_images = np.array(real_images)\n",
    "    labels = np.array([0]*len(forge_images) + [1]*len(real_images))\n",
    "    images = np.concatenate((forge_images, real_images), axis=0)\n",
    "    return images, labels\n",
    "\n",
    "# Load and normalize the dataset\n",
    "dataset_dir = \"Dataset\"\n",
    "images, labels = load_dataset(dataset_dir)\n",
    "images = images / 255.0  # Normalize images\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to build the Signet model\n",
    "def build_signet_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(96, (11, 11), strides=(4, 4), activation='relu')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Conv2D(256, (5, 5), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Conv2D(384, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(384, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "input_shape = (155, 220, 1)\n",
    "sig_model = build_signet_model(input_shape)\n",
    "\n",
    "# Function to build the Siamese network\n",
    "def siamese_model(base_model, input_shape):\n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    processed_a = base_model(input_a)\n",
    "    processed_b = base_model(input_b)\n",
    "    L1_layer = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]), output_shape=lambda input_shapes: input_shapes[0])\n",
    "    L1_distance = L1_layer([processed_a, processed_b])\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "    model = Model(inputs=[input_a, input_b], outputs=prediction)\n",
    "    return model\n",
    "\n",
    "siamese_net = siamese_model(sig_model, input_shape)\n",
    "siamese_net.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Function to generate pairs of images for training\n",
    "def make_pairs(images, labels):\n",
    "    pair_images = []\n",
    "    pair_labels = []\n",
    "    num_classes = len(np.unique(labels))\n",
    "    idx = [np.where(labels == i)[0] for i in range(num_classes)]\n",
    "    for idxA in range(len(images)):\n",
    "        current_image = images[idxA]\n",
    "        label = labels[idxA]\n",
    "        idxB = np.random.choice(idx[label])\n",
    "        pos_image = images[idxB]\n",
    "        pair_images.append([current_image, pos_image])\n",
    "        pair_labels.append([1])\n",
    "        neg_label = np.random.choice(list(set(range(num_classes)) - set([label])))\n",
    "        idxB = np.random.choice(idx[neg_label])\n",
    "        neg_image = images[idxB]\n",
    "        pair_images.append([current_image, neg_image])\n",
    "        pair_labels.append([0])\n",
    "    return np.array(pair_images), np.array(pair_labels)\n",
    "\n",
    "pairs_train, labels_train = make_pairs(X_train, y_train)\n",
    "pairs_test, labels_test = make_pairs(X_test, y_test)\n",
    "\n",
    "# Train the model\n",
    "siamese_net.fit([pairs_train[:, 0], pairs_train[:, 1]], labels_train, validation_data=([pairs_test[:, 0], pairs_test[:, 1]], labels_test), batch_size=32, epochs=10)\n",
    "siamese_net.save('signet_model.h5')\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, color_mode='grayscale', target_size=(155, 220))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "model = load_model('signet_model.h5', custom_objects={'tf': tf})\n",
    "\n",
    "# Function to evaluate the genuineness of two signatures\n",
    "def evaluate_signature(img1_path, img2_path):\n",
    "    img1 = preprocess_image(img1_path)\n",
    "    img2 = preprocess_image(img2_path)\n",
    "    prediction = model.predict([img1, img2])\n",
    "    return prediction[0][0] * 100\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "Exception encountered when calling Lambda.call().\n\n\u001b[1mname 'tf' is not defined\u001b[0m\n\nArguments received by Lambda.call():\n  • inputs=['tf.Tensor(shape=(1, 128), dtype=float32)', 'tf.Tensor(shape=(1, 128), dtype=float32)']\n  • mask=['None', 'None']\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m fake_img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignature1.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     27\u001b[0m real_img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignature2.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 28\u001b[0m genuineness \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_img_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_img_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenuineness: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenuineness\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m, in \u001b[0;36mevaluate_signature\u001b[0;34m(img1_path, img2_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m img1 \u001b[38;5;241m=\u001b[39m preprocess_image(img1_path)\n\u001b[1;32m     21\u001b[0m img2 \u001b[38;5;241m=\u001b[39m preprocess_image(img2_path)\n\u001b[0;32m---> 22\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prediction[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[7], line 68\u001b[0m, in \u001b[0;36msiamese_model.<locals>.<lambda>\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m     66\u001b[0m processed_a \u001b[38;5;241m=\u001b[39m base_model(input_a)\n\u001b[1;32m     67\u001b[0m processed_b \u001b[38;5;241m=\u001b[39m base_model(input_b)\n\u001b[0;32m---> 68\u001b[0m L1_layer \u001b[38;5;241m=\u001b[39m Lambda(\u001b[38;5;28;01mlambda\u001b[39;00m tensors: \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mabs(tensors[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m tensors[\u001b[38;5;241m1\u001b[39m]), output_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m input_shapes: input_shapes[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     69\u001b[0m L1_distance \u001b[38;5;241m=\u001b[39m L1_layer([processed_a, processed_b])\n\u001b[1;32m     70\u001b[0m prediction \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(L1_distance)\n",
      "\u001b[0;31mNameError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mname 'tf' is not defined\u001b[0m\n\nArguments received by Lambda.call():\n  • inputs=['tf.Tensor(shape=(1, 128), dtype=float32)', 'tf.Tensor(shape=(1, 128), dtype=float32)']\n  • mask=['None', 'None']\n  • training=False"
     ]
    }
   ],
   "source": [
    "# Custom function for the Lambda layer\n",
    "def custom_abs_diff(tensors):\n",
    "    x, y = tensors\n",
    "    return tf.abs(x - y)\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, color_mode='grayscale', target_size=(155, 220))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# Ensure custom Lambda function is registered during model loading\n",
    "custom_objects = {'custom_abs_diff': custom_abs_diff}\n",
    "\n",
    "model = load_model('signet_model.h5', custom_objects=custom_objects)\n",
    "\n",
    "# Function to evaluate the genuineness of two signatures\n",
    "def evaluate_signature(img1_path, img2_path):\n",
    "    img1 = preprocess_image(img1_path)\n",
    "    img2 = preprocess_image(img2_path)\n",
    "    prediction = model.predict([img1, img2])\n",
    "    return prediction[0][0] * 100\n",
    "\n",
    "# Example usage\n",
    "fake_img_path = 'signature1.jpeg'\n",
    "real_img_path = 'signature2.jpeg'\n",
    "genuineness = evaluate_signature(fake_img_path, real_img_path)\n",
    "print(f\"Genuineness: {genuineness:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 450ms/step - accuracy: 0.4739 - loss: 0.7110 - val_accuracy: 0.4861 - val_loss: 0.6932\n",
      "Epoch 2/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 452ms/step - accuracy: 0.4960 - loss: 0.6937 - val_accuracy: 0.5208 - val_loss: 0.6931\n",
      "Epoch 3/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 449ms/step - accuracy: 0.4748 - loss: 0.6932 - val_accuracy: 0.4965 - val_loss: 0.6932\n",
      "Epoch 4/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 448ms/step - accuracy: 0.5097 - loss: 0.6939 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 5/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 447ms/step - accuracy: 0.5084 - loss: 0.6940 - val_accuracy: 0.5000 - val_loss: 0.6943\n",
      "Epoch 6/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 445ms/step - accuracy: 0.4814 - loss: 0.6951 - val_accuracy: 0.4896 - val_loss: 0.6940\n",
      "Epoch 7/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 443ms/step - accuracy: 0.5068 - loss: 0.6935 - val_accuracy: 0.4965 - val_loss: 0.6932\n",
      "Epoch 8/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 445ms/step - accuracy: 0.5277 - loss: 0.6954 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 443ms/step - accuracy: 0.5084 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 10/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 444ms/step - accuracy: 0.4824 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6932\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The `{arg_name}` of this `Lambda` layer is a Python lambda. Deserializing it is unsafe. If you trust the source of the config artifact, you can override this error by passing `safe_mode=False` to `from_config()`, or calling `keras.config.enable_unsafe_deserialization().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 120\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Ensure custom Lambda function is registered during model loading\u001b[39;00m\n\u001b[1;32m    118\u001b[0m custom_objects \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_abs_diff\u001b[39m\u001b[38;5;124m'\u001b[39m: custom_abs_diff}\n\u001b[0;32m--> 120\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msignet_model.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Function to evaluate the genuineness of two signatures\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_signature\u001b[39m(img1_path, img2_path):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/saving/saving_api.py:176\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    173\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    184\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    185\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:152\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:170\u001b[0m, in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 170\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/saving/serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/models/model.py:517\u001b[0m, in \u001b[0;36mModel.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/models/functional.py:517\u001b[0m, in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# First, we create all layers and enqueue nodes to be processed\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_data \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 517\u001b[0m     \u001b[43mprocess_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# is repeated until all nodes are processed.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unprocessed_nodes:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/models/functional.py:501\u001b[0m, in \u001b[0;36mfunctional_from_config.<locals>.process_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    497\u001b[0m     layer \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[1;32m    498\u001b[0m         layer_data, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 501\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m created_layers[layer_name] \u001b[38;5;241m=\u001b[39m layer\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Gather layer inputs.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/saving/serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/layers/core/lambda_layer.py:209\u001b[0m, in \u001b[0;36mLambda.from_config\u001b[0;34m(cls, config, custom_objects, safe_mode)\u001b[0m\n\u001b[1;32m    203\u001b[0m fn_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(fn_config, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fn_config\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m fn_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__lambda__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m ):\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_for_lambda_deserialization\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     inner_config \u001b[38;5;241m=\u001b[39m fn_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    211\u001b[0m     fn \u001b[38;5;241m=\u001b[39m python_utils\u001b[38;5;241m.\u001b[39mfunc_load(\n\u001b[1;32m    212\u001b[0m         inner_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    213\u001b[0m         defaults\u001b[38;5;241m=\u001b[39minner_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefaults\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    214\u001b[0m         closure\u001b[38;5;241m=\u001b[39minner_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosure\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    215\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/layers/core/lambda_layer.py:172\u001b[0m, in \u001b[0;36mLambda._raise_for_lambda_deserialization\u001b[0;34m(arg_name, safe_mode)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_for_lambda_deserialization\u001b[39m(arg_name, safe_mode):\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m safe_mode:\n\u001b[0;32m--> 172\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `\u001b[39m\u001b[38;5;132;01m{arg_name}\u001b[39;00m\u001b[38;5;124m` of this `Lambda` layer is a Python lambda. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeserializing it is unsafe. If you trust the source of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig artifact, you can override this error \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby passing `safe_mode=False` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto `from_config()`, or calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.config.enable_unsafe_deserialization().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The `{arg_name}` of this `Lambda` layer is a Python lambda. Deserializing it is unsafe. If you trust the source of the config artifact, you can override this error by passing `safe_mode=False` to `from_config()`, or calling `keras.config.enable_unsafe_deserialization()."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 0.0001}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     90\u001b[0m siamese_net \u001b[38;5;241m=\u001b[39m siamese_model(sig_model, input_shape)\n\u001b[0;32m---> 91\u001b[0m siamese_net\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Adjusted learning rate\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Function to generate pairs of images for training\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_pairs\u001b[39m(images, labels):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/optimizers/adam.py:62\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     45\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     61\u001b[0m ):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_scale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1 \u001b[38;5;241m=\u001b[39m beta_1\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2 \u001b[38;5;241m=\u001b[39m beta_2\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py:22\u001b[0m, in \u001b[0;36mTFOptimizer.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:37\u001b[0m, in \u001b[0;36mBaseOptimizer.__init__\u001b[0;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     name \u001b[38;5;241m=\u001b[39m auto_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Argument(s) not recognized: {'lr': 0.0001}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to load images from a folder\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = load_img(os.path.join(folder, filename), color_mode='grayscale', target_size=(155, 220))\n",
    "        if img is not None:\n",
    "            images.append(img_to_array(img))\n",
    "    return np.array(images)\n",
    "\n",
    "# Function to load the dataset\n",
    "def load_dataset(main_directory):\n",
    "    forge_images = []\n",
    "    real_images = []\n",
    "    for i in range(1, 5):\n",
    "        forge_folder = os.path.join(main_directory, f'dataset{i}', 'forge')\n",
    "        real_folder = os.path.join(main_directory, f'dataset{i}', 'real')\n",
    "        forge_images.extend(load_images_from_folder(forge_folder))\n",
    "        real_images.extend(load_images_from_folder(real_folder))\n",
    "    forge_images = np.array(forge_images)\n",
    "    real_images = np.array(real_images)\n",
    "    labels = np.array([0]*len(forge_images) + [1]*len(real_images))\n",
    "    images = np.concatenate((forge_images, real_images), axis=0)\n",
    "    return images, labels\n",
    "\n",
    "# Load and normalize the dataset\n",
    "dataset_dir = \"Dataset\"\n",
    "images, labels = load_dataset(dataset_dir)\n",
    "images = images / 255.0  # Normalize images\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to build the Signet model\n",
    "def build_signet_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(96, (11, 11), strides=(4, 4), activation='relu')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Conv2D(256, (5, 5), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Conv2D(384, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(384, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "input_shape = (155, 220, 1)\n",
    "sig_model = build_signet_model(input_shape)\n",
    "\n",
    "# Custom function for the Lambda layer\n",
    "def custom_abs_diff(tensors):\n",
    "    x, y = tensors\n",
    "    return tf.abs(x - y)\n",
    "\n",
    "# Function to build the Siamese network\n",
    "def siamese_model(base_model, input_shape):\n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    processed_a = base_model(input_a)\n",
    "    processed_b = base_model(input_b)\n",
    "    L1_layer = Lambda(custom_abs_diff)\n",
    "    L1_distance = L1_layer([processed_a, processed_b])\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "    model = Model(inputs=[input_a, input_b], outputs=prediction)\n",
    "    return model\n",
    "\n",
    "siamese_net = siamese_model(sig_model, input_shape)\n",
    "siamese_net.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Function to generate pairs of images for training\n",
    "def make_pairs(images, labels):\n",
    "    pair_images = []\n",
    "    pair_labels = []\n",
    "    num_classes = len(np.unique(labels))\n",
    "    idx = [np.where(labels == i)[0] for i in range(num_classes)]\n",
    "    for idxA in range(len(images)):\n",
    "        current_image = images[idxA]\n",
    "        label = labels[idxA]\n",
    "        idxB = np.random.choice(idx[label])\n",
    "        pos_image = images[idxB]\n",
    "        pair_images.append([current_image, pos_image])\n",
    "        pair_labels.append(1)\n",
    "        neg_label = np.random.choice(list(set(range(num_classes)) - set([label])))\n",
    "        idxB = np.random.choice(idx[neg_label])\n",
    "        neg_image = images[idxB]\n",
    "        pair_images.append([current_image, neg_image])\n",
    "        pair_labels.append(0)\n",
    "    return np.array(pair_images), np.array(pair_labels)\n",
    "\n",
    "pairs_train, labels_train = make_pairs(X_train, y_train)\n",
    "pairs_test, labels_test = make_pairs(X_test, y_test)\n",
    "\n",
    "# Train the model\n",
    "siamese_net.fit([pairs_train[:, 0], pairs_train[:, 1]], labels_train, validation_data=([pairs_test[:, 0], pairs_test[:, 1]], labels_test), batch_size=32, epochs=10)\n",
    "siamese_net.save('signet_model.keras')\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, color_mode='grayscale', target_size=(155, 220))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# Ensure custom Lambda function is registered during model loading\n",
    "custom_objects = {'custom_abs_diff': custom_abs_diff}\n",
    "\n",
    "# Load the model with custom objects\n",
    "model = load_model('signet_model.keras', custom_objects=custom_objects, compile=False)\n",
    "\n",
    "# Function to evaluate the genuineness of two signatures\n",
    "def evaluate_signature(img1_path, img2_path):\n",
    "    img1 = preprocess_image(img1_path)\n",
    "    img2 = preprocess_image(img2_path)\n",
    "    prediction = model.predict([img1, img2])\n",
    "    return prediction[0][0] * 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Genuineness: 50.12%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "fake_img_path = 'signature1.jpeg'\n",
    "real_img_path = 'signature3.jpeg'\n",
    "genuineness = evaluate_signature(fake_img_path, real_img_path)\n",
    "print(f\"Genuineness: {genuineness:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 01:05:06.369221: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-16 01:05:06.452947: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 268ms/step - accuracy: 0.4836 - loss: 0.9687 - val_accuracy: 0.5020 - val_loss: 0.7364 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 264ms/step - accuracy: 0.4895 - loss: 0.8973 - val_accuracy: 0.4918 - val_loss: 0.7888 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 273ms/step - accuracy: 0.5023 - loss: 0.8924 - val_accuracy: 0.5327 - val_loss: 0.7211 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 273ms/step - accuracy: 0.5167 - loss: 0.8365 - val_accuracy: 0.5102 - val_loss: 0.8145 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 264ms/step - accuracy: 0.4611 - loss: 0.8750 - val_accuracy: 0.4592 - val_loss: 0.7465 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 265ms/step - accuracy: 0.4885 - loss: 0.7901 - val_accuracy: 0.4490 - val_loss: 0.8208 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 264ms/step - accuracy: 0.5216 - loss: 0.7826 - val_accuracy: 0.4918 - val_loss: 0.7086 - learning_rate: 2.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.5103 - loss: 0.7552 - val_accuracy: 0.4980 - val_loss: 0.7033 - learning_rate: 2.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.5080 - loss: 0.7621 - val_accuracy: 0.4837 - val_loss: 0.6971 - learning_rate: 2.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.5158 - loss: 0.7565 - val_accuracy: 0.4735 - val_loss: 0.7025 - learning_rate: 2.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 264ms/step - accuracy: 0.5081 - loss: 0.7509 - val_accuracy: 0.4918 - val_loss: 0.6964 - learning_rate: 2.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 265ms/step - accuracy: 0.4962 - loss: 0.7626 - val_accuracy: 0.5184 - val_loss: 0.6950 - learning_rate: 2.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 267ms/step - accuracy: 0.5019 - loss: 0.7567 - val_accuracy: 0.5000 - val_loss: 0.7080 - learning_rate: 2.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 263ms/step - accuracy: 0.5128 - loss: 0.7350 - val_accuracy: 0.5265 - val_loss: 0.6944 - learning_rate: 2.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.5148 - loss: 0.7426 - val_accuracy: 0.4735 - val_loss: 0.7092 - learning_rate: 2.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.5002 - loss: 0.7369 - val_accuracy: 0.5020 - val_loss: 0.6993 - learning_rate: 2.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 260ms/step - accuracy: 0.5275 - loss: 0.7304 - val_accuracy: 0.4959 - val_loss: 0.7273 - learning_rate: 2.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 263ms/step - accuracy: 0.5619 - loss: 0.7033 - val_accuracy: 0.4980 - val_loss: 0.6937 - learning_rate: 4.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.5092 - loss: 0.7442 - val_accuracy: 0.5102 - val_loss: 0.6942 - learning_rate: 4.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.5258 - loss: 0.7266 - val_accuracy: 0.5327 - val_loss: 0.6947 - learning_rate: 4.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 259ms/step - accuracy: 0.5109 - loss: 0.7307 - val_accuracy: 0.5041 - val_loss: 0.6957 - learning_rate: 4.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 260ms/step - accuracy: 0.5267 - loss: 0.7242 - val_accuracy: 0.5347 - val_loss: 0.6942 - learning_rate: 3.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 263ms/step - accuracy: 0.5088 - loss: 0.7292 - val_accuracy: 0.5061 - val_loss: 0.6903 - learning_rate: 3.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 263ms/step - accuracy: 0.5326 - loss: 0.7119 - val_accuracy: 0.5061 - val_loss: 0.6925 - learning_rate: 3.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.5263 - loss: 0.7261 - val_accuracy: 0.5184 - val_loss: 0.6935 - learning_rate: 3.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.5137 - loss: 0.7341 - val_accuracy: 0.5122 - val_loss: 0.6930 - learning_rate: 3.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.4979 - loss: 0.7378 - val_accuracy: 0.5204 - val_loss: 0.6941 - learning_rate: 3.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.5357 - loss: 0.7180 - val_accuracy: 0.4980 - val_loss: 0.6931 - learning_rate: 3.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.4992 - loss: 0.7356 - val_accuracy: 0.4857 - val_loss: 0.6959 - learning_rate: 3.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 263ms/step - accuracy: 0.5216 - loss: 0.7206 - val_accuracy: 0.5184 - val_loss: 0.6954 - learning_rate: 3.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 267ms/step - accuracy: 0.5008 - loss: 0.7284 - val_accuracy: 0.5327 - val_loss: 0.6957 - learning_rate: 3.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 263ms/step - accuracy: 0.4959 - loss: 0.7356 - val_accuracy: 0.5245 - val_loss: 0.6965 - learning_rate: 3.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.5003 - loss: 0.7420 - val_accuracy: 0.5245 - val_loss: 0.6975 - learning_rate: 3.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.4977 - loss: 0.7277 - val_accuracy: 0.5204 - val_loss: 0.6942 - learning_rate: 3.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 259ms/step - accuracy: 0.5195 - loss: 0.7302 - val_accuracy: 0.5143 - val_loss: 0.6963 - learning_rate: 3.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.5293 - loss: 0.7069 - val_accuracy: 0.5429 - val_loss: 0.6918 - learning_rate: 3.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.5015 - loss: 0.7414 - val_accuracy: 0.5510 - val_loss: 0.6956 - learning_rate: 3.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.5277 - loss: 0.7134 - val_accuracy: 0.5163 - val_loss: 0.6937 - learning_rate: 3.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.5085 - loss: 0.7294 - val_accuracy: 0.5327 - val_loss: 0.6959 - learning_rate: 3.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.5320 - loss: 0.7166 - val_accuracy: 0.5265 - val_loss: 0.6925 - learning_rate: 3.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 263ms/step - accuracy: 0.5063 - loss: 0.7394 - val_accuracy: 0.5306 - val_loss: 0.6931 - learning_rate: 3.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.5351 - loss: 0.7129 - val_accuracy: 0.5429 - val_loss: 0.6962 - learning_rate: 3.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 260ms/step - accuracy: 0.5183 - loss: 0.7193 - val_accuracy: 0.5143 - val_loss: 0.6967 - learning_rate: 3.0000e-05\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "def load_images_from_folder(folder, augmentation=False):\n",
    "    images = []\n",
    "    if augmentation:\n",
    "        datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1,\n",
    "                                     shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest')\n",
    "        for filename in os.listdir(folder):\n",
    "            img = load_img(os.path.join(folder, filename), color_mode='grayscale', target_size=(155, 220))\n",
    "            if img is not None:\n",
    "                img = img_to_array(img)\n",
    "                img = np.expand_dims(img, axis=0)\n",
    "                for batch in datagen.flow(img, batch_size=1):\n",
    "                    images.append(batch[0])\n",
    "                    break\n",
    "    else:\n",
    "        for filename in os.listdir(folder):\n",
    "            img = load_img(os.path.join(folder, filename), color_mode='grayscale', target_size=(155, 220))\n",
    "            if img is not None:\n",
    "                images.append(img_to_array(img))\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "def load_datasets(main_directories, augmentation=False):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    for main_directory in main_directories:\n",
    "        forge_images = []\n",
    "        real_images = []\n",
    "        datasets = ['dataset1', 'dataset2', 'dataset3', 'dataset4']\n",
    "        for dataset in datasets:\n",
    "            forge_folder = os.path.join(main_directory, dataset, 'forge')\n",
    "            real_folder = os.path.join(main_directory, dataset, 'real')\n",
    "            forge_images.extend(load_images_from_folder(forge_folder, augmentation))\n",
    "            real_images.extend(load_images_from_folder(real_folder, augmentation))\n",
    "        forge_images = np.array(forge_images)\n",
    "        real_images = np.array(real_images)\n",
    "        labels = np.array([0] * len(forge_images) + [1] * len(real_images))\n",
    "        images = np.concatenate((forge_images, real_images), axis=0)\n",
    "        all_images.append(images)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    all_images = np.concatenate(all_images, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    return all_images, all_labels\n",
    "\n",
    "\n",
    "# Define main directories where datasets are located\n",
    "main_directories = [\n",
    "    \"archive (1)/Dataset_Signature_Final/Dataset\",\n",
    "    \"archive (1)/dataset_signature_final/Dataset\"\n",
    "]\n",
    "images, labels = load_datasets(main_directories, augmentation=True)\n",
    "images = images / 255.0  # Normalize images\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(images, labels, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to build the Signet model with enhancements\n",
    "def build_signet_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (11, 11), strides=(4, 4), activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Conv2D(128, (5, 5), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "input_shape = (155, 220, 1)\n",
    "sig_model = build_signet_model(input_shape)\n",
    "\n",
    "# Custom function for the Lambda layer\n",
    "def custom_abs_diff(tensors):\n",
    "    x, y = tensors\n",
    "    return tf.abs(x - y)\n",
    "\n",
    "def siamese_model(base_model, input_shape):\n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    processed_a = base_model(input_a)\n",
    "    processed_b = base_model(input_b)\n",
    "    L1_layer = Lambda(custom_abs_diff)([processed_a, processed_b])\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_layer)\n",
    "    model = Model(inputs=[input_a, input_b], outputs=prediction)\n",
    "    return model\n",
    "\n",
    "\n",
    "siamese_net = siamese_model(sig_model, input_shape)\n",
    "siamese_net.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Function to generate pairs of images for training\n",
    "def make_pairs(images, labels):\n",
    "    pair_images = []\n",
    "    pair_labels = []\n",
    "    num_classes = len(np.unique(labels))\n",
    "    idx = [np.where(labels == i)[0] for i in range(num_classes)]\n",
    "    for idxA in range(len(images)):\n",
    "        current_image = images[idxA]\n",
    "        label = labels[idxA]\n",
    "        # Positive Pair\n",
    "        idxB = np.random.choice(idx[label])\n",
    "        pos_image = images[idxB]\n",
    "        pair_images.append([current_image, pos_image])\n",
    "        pair_labels.append(1)\n",
    "        # Negative Pair\n",
    "        neg_label = np.random.choice(list(set(range(num_classes)) - set([label])))\n",
    "        idxB = np.random.choice(idx[neg_label])\n",
    "        neg_image = images[idxB]\n",
    "        pair_images.append([current_image, neg_image])\n",
    "        pair_labels.append(0)\n",
    "    return np.array(pair_images), np.array(pair_labels)\n",
    "\n",
    "pairs_train, labels_train = make_pairs(X_train, y_train)\n",
    "pairs_val, y_val = make_pairs(X_val, y_val)\n",
    "pairs_test, labels_test = make_pairs(X_test, y_test)\n",
    "\n",
    "# Enhanced training with callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00003)\n",
    "history = siamese_net.fit(\n",
    "    [pairs_train[:, 0], pairs_train[:, 1]], labels_train, \n",
    "    validation_data=([pairs_val[:, 0], pairs_val[:, 1]], y_val), \n",
    "    batch_size=32, epochs=50, \n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "siamese_net.save('signet_model_augmented.keras')\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, color_mode='grayscale', target_size=(155, 220))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# Ensure custom Lambda function is registered during model loading\n",
    "custom_objects = {'custom_abs_diff': custom_abs_diff}\n",
    "\n",
    "# Load the model with custom objects\n",
    "model = load_model('signet_model_augmented.keras', custom_objects=custom_objects, compile=False)\n",
    "\n",
    "# Function to evaluate the genuineness of two signatures\n",
    "def evaluate_signature(img1_path, img2_path):\n",
    "    img1 = preprocess_image(img1_path)\n",
    "    img2 = preprocess_image(img2_path)\n",
    "    prediction = model.predict([img1, img2])\n",
    "    return prediction[0][0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "fraud: 100.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "fake_img_path = 'fraud1.jpeg'\n",
    "real_img_path = 'signature1.jpeg'\n",
    "genuineness = evaluate_signature(fake_img_path, real_img_path)\n",
    "\n",
    "if genuineness>75:\n",
    "    print(f\"fraud: {genuineness:.2f}%\")\n",
    "else:\n",
    "    print(\"genuine\")\n",
    "\n",
    "# Evaluate training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
